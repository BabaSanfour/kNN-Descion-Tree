{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eccde28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.debugger import set_trace\n",
    "np.random.seed(1234)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1897434",
   "metadata": {},
   "source": [
    "# Summary and visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1301a106",
   "metadata": {},
   "source": [
    "## First Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62f4ac9",
   "metadata": {},
   "source": [
    "### 1: read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56096de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hepatitis = pd.read_csv('Datasets/hepatitis.csv')\n",
    "df_hepatitis.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23364ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the number of \"2\" labels \n",
    "print(\"Original Number of labels '2': \", list(df_hepatitis[\"Class\"]).count(2))\n",
    "# check the number of \"1\" labels\n",
    "print(\"Original Number of labels '1': \", list(df_hepatitis[\"Class\"]).count(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa75d620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace \"?\" to NAN\n",
    "df_hepatitis=df_hepatitis.replace('?', np.NaN)\n",
    "df_hepatitis.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae832e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hepatitis_cleaned=df_hepatitis.dropna(axis=0,how=\"any\")\n",
    "df_hepatitis_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9eaa5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the number of \"2\" labels \n",
    "print(\"New Number of labels '2': \", list(df_hepatitis_cleaned[\"Class\"]).count(2))\n",
    "# check the number of \"1\" labels\n",
    "print(\"New Number of labels '1': \", list(df_hepatitis_cleaned[\"Class\"]).count(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0be3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the data types\n",
    "df_hepatitis_cleaned.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a59689",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns=df_hepatitis_cleaned.columns\n",
    "print(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a688104",
   "metadata": {},
   "outputs": [],
   "source": [
    "turn_into_int_columns=['steroid', 'fatigue', 'malaise',\n",
    "       'anorexia', 'liver_big', 'liver_firm', 'spleen_palpable', 'spiders',\n",
    "       'ascites', 'varices',  'sgot', 'protime']\n",
    "for col in turn_into_int_columns:\n",
    "    df_hepatitis_cleaned[col]=df_hepatitis_cleaned[col].astype(int)\n",
    "turn_into_float_columns=['bilirubin', 'alk_phosphate', 'albumin']\n",
    "for col in turn_into_float_columns:\n",
    "    df_hepatitis_cleaned[col]=df_hepatitis_cleaned[col].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0ecc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the data types\n",
    "df_hepatitis_cleaned.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bb71db",
   "metadata": {},
   "source": [
    "### 2: the summary statistics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee7167d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the categorical features, e.g. variable id=\"SEX\"-\"VARICES\", \n",
    "height = [list(df_hepatitis_cleaned[\"sex\"]).count(1), list(df_hepatitis_cleaned[\"sex\"]).count(2)]\n",
    "bars = ('1', '2')\n",
    "y_pos = np.arange(len(bars))\n",
    "plt.bar(y_pos, height)\n",
    "plt.xticks(y_pos, bars)\n",
    "plt.ylabel('Count')\n",
    "plt.xlabel('Labels')\n",
    "plt.title('Count of labels for the sex variable')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01adc45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the label\n",
    "height = [list(df_hepatitis_cleaned[\"Class\"]).count(1), list(df_hepatitis_cleaned[\"Class\"]).count(2)]\n",
    "# 1 is for die and 2 is for live\n",
    "bars = ('1', '2')\n",
    "y_pos = np.arange(len(bars))\n",
    "plt.bar(y_pos, height)\n",
    "plt.xticks(y_pos, bars)\n",
    "plt.ylabel('Count')\n",
    "plt.xlabel('Labels')\n",
    "plt.title('Count of labels in the cleaned dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4dfcf90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are going to check the percentage of \"1\" for each (categorical) features\n",
    "fig=plt.figure(figsize=(18,4))\n",
    "plt.gca().margins(x=0.01)\n",
    "plt.gcf().canvas.draw()\n",
    "height = [list(df_hepatitis_cleaned[\"Class\"]).count(1)/(list(df_hepatitis_cleaned[\"Class\"]).count(1)+\n",
    "                                       list(df_hepatitis_cleaned[\"Class\"]).count(2)), \n",
    "         list(df_hepatitis_cleaned[\"sex\"]).count(1)/(list(df_hepatitis_cleaned[\"sex\"]).count(1)+\n",
    "                                       list(df_hepatitis_cleaned[\"sex\"]).count(2)),\n",
    "         list(df_hepatitis_cleaned[\"steroid\"]).count(1)/(list(df_hepatitis_cleaned[\"steroid\"]).count(1)+\n",
    "                                       list(df_hepatitis_cleaned[\"steroid\"]).count(2)),\n",
    "         list(df_hepatitis_cleaned[\"antivirals\"]).count(1)/(list(df_hepatitis_cleaned[\"antivirals\"]).count(1)+\n",
    "                                       list(df_hepatitis_cleaned[\"antivirals\"]).count(2)),\n",
    "         list(df_hepatitis_cleaned[\"fatigue\"]).count(1)/(list(df_hepatitis_cleaned[\"fatigue\"]).count(1)+\n",
    "                                       list(df_hepatitis_cleaned[\"fatigue\"]).count(2)),\n",
    "         list(df_hepatitis_cleaned[\"malaise\"]).count(1)/(list(df_hepatitis_cleaned[\"malaise\"]).count(1)+\n",
    "                                       list(df_hepatitis_cleaned[\"malaise\"]).count(2)),\n",
    "         list(df_hepatitis_cleaned[\"anorexia\"]).count(1)/(list(df_hepatitis_cleaned[\"anorexia\"]).count(1)+\n",
    "                                       list(df_hepatitis_cleaned[\"anorexia\"]).count(2)),\n",
    "         list(df_hepatitis_cleaned[\"liver_big\"]).count(1)/(list(df_hepatitis_cleaned[\"liver_big\"]).count(1)+\n",
    "                                       list(df_hepatitis_cleaned[\"liver_big\"]).count(2)),\n",
    "         list(df_hepatitis_cleaned[\"liver_firm\"]).count(1)/(list(df_hepatitis_cleaned[\"liver_firm\"]).count(1)+\n",
    "                                       list(df_hepatitis_cleaned[\"liver_firm\"]).count(2)),\n",
    "         list(df_hepatitis_cleaned[\"spleen_palpable\"]).count(1)/(list(df_hepatitis_cleaned[\"spleen_palpable\"]).count('1')+\n",
    "                                       list(df_hepatitis_cleaned[\"spleen_palpable\"]).count(2)),\n",
    "         list(df_hepatitis_cleaned[\"spiders\"]).count(1)/(list(df_hepatitis_cleaned[\"spiders\"]).count(1)+\n",
    "                                       list(df_hepatitis_cleaned[\"spiders\"]).count(2)),\n",
    "         list(df_hepatitis_cleaned[\"ascites\"]).count(1)/(list(df_hepatitis_cleaned[\"ascites\"]).count(1)+\n",
    "                                       list(df_hepatitis_cleaned[\"ascites\"]).count(2)),\n",
    "         list(df_hepatitis_cleaned[\"varices\"]).count(1)/(list(df_hepatitis_cleaned[\"varices\"]).count(1)+\n",
    "                                       list(df_hepatitis_cleaned[\"varices\"]).count(2)),\n",
    "         list(df_hepatitis_cleaned[\"histology\"]).count(1)/(list(df_hepatitis_cleaned[\"histology\"]).count(1)+\n",
    "                                       list(df_hepatitis_cleaned[\"histology\"]).count(2))]\n",
    "\n",
    "# 1 is for die and 2 is for live\n",
    "bars = ('Class',\"sex\",\"steroid\",\"antivirals\",\"fatigue\",\"malaise\",\"anorexia\",\"liver_big\",\"liver_firm\",\n",
    "       \"spleen_palpable\",\"spiders\",\"ascites\",\"varices\",\"histology\")\n",
    "plt.bar(bars, height,color=['r','b','b','b','b','b','b','b','b','b','b','b','b','b'])\n",
    "#plt.xlabel('Labels and categorical features')\n",
    "plt.ylabel('Percentage')\n",
    "plt.ylim((0,1))\n",
    "plt.title(\"(a) Percentage of '1' in the label and categorical features\")\n",
    "#plt.show()\n",
    "plt.savefig('data1-balance.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b743d4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the correlation heatmap between features\n",
    "df_hepatitis_features=df_hepatitis_cleaned.drop('Class',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ad8930",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df_hepatitis_features.corr())\n",
    "plt.savefig('data1-corr.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be9be52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking density plots for quantative variables\n",
    "sns.kdeplot(df_hepatitis_cleaned['age']).set_title('Density plot of age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711ed7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(df_hepatitis_cleaned['bilirubin'].astype(float)).set_title('Density plot of bilirubin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e669df",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(df_hepatitis_cleaned['alk_phosphate'].astype(float)).set_title('Density plot of alk phosphate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3a0702",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(df_hepatitis_cleaned['sgot'].astype(float)).set_title('Density plot of sgot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3500179e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(df_hepatitis_cleaned['albumin'].astype(float)).set_title('Density plot of albumin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26971e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(df_hepatitis_cleaned['protime'].astype(float)).set_title('Density plot of protime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ff84ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check some features with labels we are interested\n",
    "sns.violinplot(data=df_hepatitis_cleaned,x='Class',y='age')\n",
    "# recall '1' is for dead and '2' is for alive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2dda40",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.violinplot(data=df_hepatitis_cleaned,x='Class',y='age',hue='sex')\n",
    "# no sex=2 in class 1!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc39873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the summary statistics for the quantitive variables\n",
    "# 'age','bilirubin'\n",
    "print('Age')\n",
    "print(df_hepatitis_cleaned[['age']].describe())\n",
    "print('bilirubin')\n",
    "print(df_hepatitis_cleaned['bilirubin'].astype(float).describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78986c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'alk_phosphate','sgot'\n",
    "print('alk_phosphate')\n",
    "print(df_hepatitis_cleaned[['alk_phosphate']].astype(float).describe())\n",
    "print('sgot')\n",
    "print(df_hepatitis_cleaned['sgot'].astype(float).describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eea70ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'albumin','sgot'\n",
    "print('albumin')\n",
    "print(df_hepatitis_cleaned[['albumin']].astype(float).describe())\n",
    "print('protime')\n",
    "print(df_hepatitis_cleaned['protime'].astype(float).describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32da3813",
   "metadata": {},
   "source": [
    "## For the second dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51ce7d5",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45dc0500",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_messidor_features= pd.read_csv('Datasets/messidor_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d03302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# because of the description in \n",
    "#https://archive.ics.uci.edu/ml/datasets/Diabetic+Retinopathy+Debrecen+Data+Set\n",
    "# column 9-16) contain the same information as 3-8) , we can delete any one set of features.\n",
    "# We try to keep column 3-8) only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bbfaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_messidor_features_cleaned=df_messidor_features.drop(['exudates_1',\"exudates_2\",\"exudates_3\",\"exudates_4\",\"exudates_5\",\"exudates_6\",\"exudates_7\",\"exudates_8\"], axis=1) \n",
    "df_messidor_features_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51476152",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_messidor_features_cleaned.isnull().count(False)\n",
    "# there is no missing data in this set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca880bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check types \n",
    "df_messidor_features_cleaned.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f026e12",
   "metadata": {},
   "source": [
    "### 2: the summary statistics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5209e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quality, pre-screening , AM_FM_classification are binary features, \n",
    "# MA_detection_XX , euclidean_distance and diameter are quantative features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93de517e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# similar as previous plots, we visualize the percentage of \"0\" class\n",
    "fig=plt.figure(figsize=(18,4))\n",
    "#plt.gca().margins(x=0.01)\n",
    "#plt.gcf().canvas.draw()\n",
    "height = [list(df_messidor_features_cleaned[\"Class\"]).count(1)/(list(df_messidor_features_cleaned[\"Class\"]).count(0)+\n",
    "                                       list(df_messidor_features_cleaned[\"Class\"]).count(1)), \n",
    "         list(df_messidor_features_cleaned[\"quality\"]).count(1)/(list(df_messidor_features_cleaned[\"quality\"]).count(0)+\n",
    "                                       list(df_messidor_features_cleaned[\"quality\"]).count(1)),\n",
    "         list(df_messidor_features_cleaned[\"pre-screening\"]).count(1)/(list(df_messidor_features_cleaned[\"pre-screening\"]).count(0)+\n",
    "                                       list(df_messidor_features_cleaned[\"pre-screening\"]).count(1)),\n",
    "         list(df_messidor_features_cleaned[\"AM_FM_classification\"]).count(1)/(list(df_messidor_features_cleaned[\"AM_FM_classification\"]).count(0)+\n",
    "                                       list(df_messidor_features_cleaned[\"AM_FM_classification\"]).count(1))]\n",
    "# 1 is for die and 2 is for live\n",
    "bars = ('Class',\"quality\",\"pre-screening\",\"AM_FM_classification\")\n",
    "#y_pos = np.arange(len(bars))\n",
    "plt.bar(bars, height,color=['r','b','b','b'])\n",
    "#plt.xlabel('The label and categorical features')\n",
    "plt.ylabel('Percentage')\n",
    "plt.ylim((0,1))\n",
    "plt.title(\"(b) Percentage of '1' in the label and categorical features\")\n",
    "#plt.show()\n",
    "plt.savefig('data2-balance.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1bb67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the percentage of quality os quite small, we suspect it is useless for prediction.\n",
    "# actually the percentage is \n",
    "print(\"percentage of quality in label 0: \", list(df_messidor_features_cleaned[\"quality\"]).count(0)/(list(df_messidor_features_cleaned[\"quality\"]).count(0)+\n",
    "                                       list(df_messidor_features_cleaned[\"quality\"]).count(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d85224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the correlation heatmap between features\n",
    "df_messidor_features=df_messidor_features_cleaned.drop('Class',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d883d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df_messidor_features.corr())\n",
    "plt.savefig('data2-corr.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e79d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for quantative variables\n",
    "sns.kdeplot(df_messidor_features_cleaned['MA_detection_0.5'])\n",
    "sns.kdeplot(df_messidor_features_cleaned['MA_detection_0.6'])\n",
    "sns.kdeplot(df_messidor_features_cleaned['MA_detection_0.7'])\n",
    "sns.kdeplot(df_messidor_features_cleaned['MA_detection_0.8'])\n",
    "sns.kdeplot(df_messidor_features_cleaned['MA_detection_0.9'])\n",
    "sns.kdeplot(df_messidor_features_cleaned['MA_detection_1.0']).set_title('Density plot of number of MAs at alpha from 0.5 to 1.0')\n",
    "# they are similar to MAs at alpha=0.5, 0.6, 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0138251e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check some features with labels we are interested\n",
    "sns.violinplot(data=df_messidor_features_cleaned,x='Class',y='quality')\n",
    "# recall we doubt quality maybe not a good feature for prediction, \n",
    "#I would suggest delete this feature as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bee961",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.violinplot(data=df_messidor_features_cleaned,x='Class',y='pre-screening')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613eea48",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.violinplot(data=df_messidor_features_cleaned,x='Class',y='euclidean_distance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8534ef5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.violinplot(data=df_messidor_features_cleaned,x='Class',y='euclidean_distance',hue='AM_FM_classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95d5c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#center values around 0/1\n",
    "df_hepatitis_cleaned.sex-=1\n",
    "df_hepatitis_cleaned.steroid-=1\n",
    "df_hepatitis_cleaned.antivirals-=1\n",
    "df_hepatitis_cleaned.fatigue-=1\n",
    "df_hepatitis_cleaned.malaise-=1\n",
    "df_hepatitis_cleaned.anorexia-=1\n",
    "df_hepatitis_cleaned.liver_big-=1\n",
    "df_hepatitis_cleaned.liver_firm-=1\n",
    "df_hepatitis_cleaned.spleen_palpable-=1\n",
    "df_hepatitis_cleaned.spiders-=1\n",
    "df_hepatitis_cleaned.ascites-=1\n",
    "df_hepatitis_cleaned.varices-=1\n",
    "df_hepatitis_cleaned.histology-=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5ba276",
   "metadata": {},
   "source": [
    "## Normalize and scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44eb4eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data1\n",
    "to_normalize=np.array(df_hepatitis_cleaned[['age','bilirubin', 'alk_phosphate',\n",
    "                                           'sgot', 'albumin', 'protime']])\n",
    "\n",
    "to_normalize=preprocessing.StandardScaler().fit_transform(to_normalize)\n",
    "df_hepatitis_normalized=df_hepatitis_cleaned.copy()\n",
    "df_hepatitis_normalized['age']=to_normalize[:,0]\n",
    "df_hepatitis_normalized['bilirubin']=to_normalize[:,1]\n",
    "df_hepatitis_normalized['alk_phosphate']=to_normalize[:,2]\n",
    "df_hepatitis_normalized['sgot']=to_normalize[:,3]\n",
    "df_hepatitis_normalized['albumin']=to_normalize[:,4]\n",
    "df_hepatitis_normalized['protime']=to_normalize[:,5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2313a026",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hepatitis_normalized.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbaad47a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e0fe79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data2\n",
    "to_normalize=np.array(df_messidor_features_cleaned[['MA_detection_0.5','MA_detection_0.6', 'MA_detection_0.7',\n",
    "                                           'MA_detection_0.8', 'MA_detection_0.9', 'MA_detection_1.0',\n",
    "                                           'euclidean_distance']])\n",
    "\n",
    "to_normalize=preprocessing.StandardScaler().fit_transform(to_normalize)\n",
    "df_messidor_features_normalized=df_messidor_features_cleaned.copy()\n",
    "df_messidor_features_normalized['MA_detection_0.5']=to_normalize[:,0]\n",
    "df_messidor_features_normalized['MA_detection_0.6']=to_normalize[:,1]\n",
    "df_messidor_features_normalized['MA_detection_0.7']=to_normalize[:,2]\n",
    "df_messidor_features_normalized['MA_detection_0.8']=to_normalize[:,3]\n",
    "df_messidor_features_normalized['MA_detection_0.9']=to_normalize[:,4]\n",
    "df_messidor_features_normalized['MA_detection_1.0']=to_normalize[:,5]\n",
    "df_messidor_features_normalized['euclidean_distance']=to_normalize[:,6]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2788089",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_messidor_features_normalized.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea9345e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Above we Standarized the all the data manually without taking into account the train/test splits or \n",
    "#cross val! later after splitting the data we will use the function below to normalize each set on its own\n",
    "# to avoid data leak\n",
    "\n",
    "def Standarize( dataset, name, columns_index=None):\n",
    "    if columns_index==None:\n",
    "        if name=='hepatitis':\n",
    "            columns_index=[0,13, 14, 15, 16, 17]\n",
    "        else:\n",
    "            columns_index= [1,2,3,4,5,6,7]\n",
    "        to_normalize=preprocessing.StandardScaler().fit_transform(dataset[:,columns_index])\n",
    "        for i in range(len(columns_index)):\n",
    "            dataset[:,columns_index[i]]=to_normalize[:,i]\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e94b610",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = Standarize(np.array(df_hepatitis_cleaned.drop('Class', axis=1)), 'hepatitis')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52eafe6",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da4489f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_acc(y_test, preds):\n",
    "    return np.sum(preds == y_test)/y_test.shape[0]\n",
    "\n",
    "def confusion_matrix(y_test, preds, labels=[0,1]):\n",
    "    mask_true=np.array(y_test==labels[1]) # actually +\n",
    "    mask_false=np.array(y_test==labels[0]) # actually -\n",
    "    tn = list(preds[mask_false]).count(labels[0])\n",
    "    fn = list(preds[mask_true]).count(labels[0])\n",
    "    tp = list(preds[mask_true]).count(labels[1])\n",
    "    fp = list(preds[mask_false]).count(labels[1])\n",
    "    return np.array([[tn, fp], [fn, tp]])\n",
    "\n",
    "def sensitivity(conf_matrix):\n",
    "    tp = conf_matrix[1][1]\n",
    "    fn = conf_matrix[1][0]\n",
    "    return tp/(tp+fn)\n",
    "\n",
    "def precision(conf_matrix):\n",
    "    tp = conf_matrix[1][1]\n",
    "    fp = conf_matrix[0][1]\n",
    "    return tp/(tp+fp)\n",
    "\n",
    "def fpr(conf_matrix):   \n",
    "    fp = conf_matrix[0][1]\n",
    "    tn = conf_matrix[0][0]\n",
    "    return fp/(fp+tn)\n",
    "\n",
    "def specificity(conf_matrix):\n",
    "    # true negative rate\n",
    "    # tn / (fp+tn)\n",
    "    tn = conf_matrix[0][0]\n",
    "    fp = conf_matrix[0][1]\n",
    "    return tn/(tn+fp)\n",
    "\n",
    "def f1_score(prec, sensi):\n",
    "    # recall is sensitivity\n",
    "    # 2 * (precision * recall) / (precision + recall)\n",
    "    return (2*prec*sensi)/(prec + sensi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b40c50a",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2922d36",
   "metadata": {},
   "source": [
    "## KNN Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f5aca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two distance functions for kNN\n",
    "euclidean = lambda x1, x2: np.sqrt(np.sum((x1 - x2)**2, axis=-1))\n",
    "manhattan = lambda x1, x2: np.sum(np.abs(x1 - x2), axis=-1)\n",
    "\n",
    "def cosine_similarity(train, test):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        train:  np.array of training points\n",
    "        test:   np.array of test points\n",
    "    Returns: \n",
    "        similarity matrix of length (num_test by num_train), containing similarities between all test & train points\n",
    "    \"\"\"\n",
    "    all_sims = np.zeros((len(test), len(train)))    # we will store all the similarities\n",
    "    for i in range(len(test)):                      # go through each test point                \n",
    "        for j in range(len(train)):                 # Compare the test point with all training points \n",
    "            all_sims[i][j] = np.sum(test[i] * train[j]) / ( np.sqrt(np.sum(test[i]**2)) * np.sqrt(np.sum(train[j]**2)) )\n",
    "    return all_sims\n",
    "\n",
    "def function_type(func):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        func:    function\n",
    "    Returns: \n",
    "        string indicating whether the input function was a distance or similarity function\n",
    "    \"\"\"\n",
    "    if (func in [euclidean, manhattan]):\n",
    "        return \"dist\"\n",
    "    else:\n",
    "        return \"sim\"\n",
    "\n",
    "#count occurence of a class in a set of test points\n",
    "def knn_count(neigbourhood_labels, num_classes):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        neigbourhood_labels:    np.array containing labels of all knn \n",
    "        num_classes:            integer, tells the total number of classes in dataset\n",
    "    Returns: \n",
    "        np.array of size 1 by num_classes, which indicates how many labels of \n",
    "        each class was found in neigbourhood\n",
    "    \"\"\"\n",
    "    counts_array, neigbourhood_labels = [], list(neigbourhood_labels)\n",
    "    for c in range(num_classes):    # go through each label\n",
    "        counts_array.append(neigbourhood_labels.count(c))   # measure the quantity of the class, and place in array\n",
    "    return np.array(counts_array)     \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642d6332",
   "metadata": {},
   "source": [
    "## KNN Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee606a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class kNN():\n",
    "    \"\"\"\n",
    "    K Nearest Neighbors Class\n",
    "    \"\"\"\n",
    "    def __init__(self, k, dist_func, func_type, weighted: bool = False):\n",
    "        self.k = k\n",
    "        self.dist_func = dist_func  # have a variable distance function\n",
    "        self.func_type = func_type  # string of if the function is distance (\"dist\") or similarity (\"sim\")\n",
    "        # here, we added the .func_type\n",
    "        self.weighted = weighted\n",
    "    \n",
    "    def fit(self, X_train, y_train):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            X_train:    np.array, features of all training data points\n",
    "            y_train:    np.array, labels of all training points\n",
    "        Returns: \n",
    "            Nothing\n",
    "        \"\"\"\n",
    "        self.X_train = X_train\n",
    "        if list(np.unique(y_train)) == [0,1]:\n",
    "            self.y_train = y_train                   # \"center\" the labels at 0\n",
    "        else:\n",
    "            self.y_train = y_train - 1            \n",
    "        self.num_classes = len(np.unique(y_train))  # number of classes\n",
    "        self.classes = list(np.unique(y_train))\n",
    "        \n",
    "    def predict(self, test_data: np.array, prob: bool = False):\n",
    "        \"\"\" Prediction function:  \n",
    "        Input: \n",
    "            test_data:   np.array, Data we want to predict its class.\n",
    "            prob:        bool, if True return class probilities else a vector of predictions\n",
    "        Output:          class probilities else a vector of predictions. \n",
    "        \"\"\"           \n",
    "        # Step 1: initalize output arrays\n",
    "        knns = np.zeros((test_data.shape[0], self.k), dtype=int)    # empty k nearest neigbours of shape (num points, k)\n",
    "        y_prob = np.zeros((test_data.shape[0], self.num_classes))   # prob distributions over classes\n",
    "\n",
    "        # Step 2: find the K nearest points, accoriding to function type\n",
    "        # This if/else was added by us to work for both distance and similarity\n",
    "        if (self.func_type == \"dist\"):\n",
    "            multiplyer = 1  # find SMALLEST k\n",
    "            distances = self.dist_func(self.X_train[None,:,:], test_data[:,None,:]) # calc all distances\n",
    "        else:\n",
    "            multiplyer = -1 # find LARGEST k\n",
    "            distances = self.dist_func(self.X_train, test_data) # calc all similarities\n",
    "\n",
    "        for i in range(test_data.shape[0]): # go through all testing point\n",
    "            # we must use the multiplyer to get the proper ordering\n",
    "            knns[i,:] = np.argsort(multiplyer * distances[i])[:self.k] # sort the distances and store k sized slice\n",
    "            \n",
    "            # this was added by us (the weighted knn)\n",
    "            if self.weighted:\n",
    "                # this was also added by us\n",
    "                # similarity is defined as either the inverse of the Euclidean distance or cosine similarity\n",
    "                if (self.func_type == \"dist\"):  # if we use distance function, we will use 1/distance\n",
    "                    similarity = 1/distances[i]\n",
    "                    similarity[similarity == float('inf')] = 999999 # prevents divide by zero errors\n",
    "                else:   # if we use similarity, we will continue to use similarity for weighted\n",
    "                    similarity = distances[i]\n",
    "                    similarity[similarity == float('inf')] = 999999 # prevents divide by zero errors\n",
    "                for c in range(self.num_classes):\n",
    "                    mask=np.array(self.y_train[knns[i,:]]==c) #get the positions of neighbors that are from class c \n",
    "                    y_prob[i,:][c]=np.sum(similarity[knns[i,:]][mask]) #get the the sum of the distances of those neighbors\n",
    "                    # and assign them y_prob\n",
    "                y_prob[i,:]=(y_prob[i,:])/np.sum(similarity[knns[i,:]]) # / by the sum of similarities\n",
    "            else: \n",
    "                # if unweighted, we just count the quantity of each class among the neigbourhood points\n",
    "                y_prob[i,:] = knn_count(self.y_train[knns[i,:]], num_classes = self.num_classes) \n",
    "        if not self.weighted:\n",
    "            y_prob /= self.k # if not weighted divide by k\n",
    "\n",
    "        # Step 3: return probilities or classes\n",
    "        # this was added (so that it could give a hard prediction or the probability of each class)\n",
    "        if prob:\n",
    "            return y_prob\n",
    "        if self.classes == [0,1]: #work with [2,1] and [1,0] labels\n",
    "            return np.argmax(y_prob,1)\n",
    "        else:\n",
    "            return np.argmax(y_prob,1)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ee71e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class kNNVars:\n",
    "    def __init__(self, k = None, f = None, cf = None, mli = None, pa = None):\n",
    "        self.k = k\n",
    "        self.func = f\n",
    "        self.pred_ac = pa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73888fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_knn_best_params(range_of_neighbors, train_data, train_data_y, validation_data, val_data_y, validate_functions, weighted=False):\n",
    "    \"\"\"\n",
    "    Input\n",
    "        range_of_neighbors     numpy array of the range of neighbors we will test to find the best\n",
    "        train_data             the data we will be using to train the possible models, as np.array\n",
    "        train_data_y           labels of train data \n",
    "        validation_data        the data we will use to validate the best k, as np.array\n",
    "        val_data_y             labels of validation data \n",
    "        validate_functions     if non empty, we validate which function is best\n",
    "        weighted               if True, use weighted kNN\n",
    " \n",
    "    Output\n",
    "        best_k                 the number of neighbors that give highest accuracy on validation set\n",
    "    \"\"\"\n",
    "    \n",
    "    variable_funcs = (len(validate_functions) != 0)\n",
    "    \n",
    "    best_variable_set = kNNVars()    # this will hold the \n",
    "    best_variable_set.pred_ac = -1  # baseline (we will update this as we find better variable acc)\n",
    "\n",
    "    for k in range_of_neighbors:   # cycle through all depths\n",
    "\n",
    "        if (variable_funcs):\n",
    "            # choose a random function to test\n",
    "            f = validate_functions[random.randint(0,len(validate_functions)-1)]\n",
    "        else:\n",
    "            f = cosine_similarity\n",
    "\n",
    "        # make knn instance\n",
    "        knn = kNN(k, f, function_type(f), weighted)\n",
    "\n",
    "        # train the knn\n",
    "        knn.fit(train_data, train_data_y)  # the labels are th y values associated with train_data features\n",
    "\n",
    "        # validate this model bu predicting labels of validation_data\n",
    "        preds = knn.predict(validation_data)\n",
    "\n",
    "        pred_accuracy = evaluate_acc(preds, val_data_y)\n",
    "\n",
    "        # if the prediction accuracy of this setup is higher than existing then we update the best_variable_set\n",
    "        if (pred_accuracy > best_variable_set.pred_ac):\n",
    "            best_variable_set.pred_ac = pred_accuracy\n",
    "            best_variable_set.k = k\n",
    "            best_variable_set.func = function_type(f)\n",
    "    return best_variable_set    # this will now hold the absolute best of the variable sets we saw\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1a1296",
   "metadata": {},
   "source": [
    "## Decision Tree Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38d5227",
   "metadata": {},
   "outputs": [],
   "source": [
    "#count occurence of a class in a leaf \n",
    "def leaf_count(labels_in_leaf, classes):\n",
    "    c, labels_in_leaf=[], list(labels_in_leaf)\n",
    "    for label in classes:\n",
    "        c.append(labels_in_leaf.count(label))\n",
    "    return np.array(c)     \n",
    "\n",
    "\n",
    "#computes misclassification cost by subtracting the maximum probability of any class\n",
    "def cost_misclassification(labels_in_leaf, classes):\n",
    "    counts = leaf_count(labels_in_leaf, classes)    # count all of the labels in the leaf node\n",
    "    class_probs = counts / np.sum(counts)   # calculate p(class | leaf)\n",
    "    return 1 - np.max(class_probs)          # return misclass, which is (1 - the \"leaf label\"), leaf label is the class with the maximum prob\n",
    "\n",
    "#computes entropy of the labels by computing the class probabilities\n",
    "def cost_entropy(labels_in_leaf, classes):\n",
    "    class_probs = leaf_count(labels_in_leaf, classes) / len(labels_in_leaf)\n",
    "    class_probs = class_probs[class_probs > 0]              # this steps is remove 0 probabilities for removing numerical issues while computing log\n",
    "    return -np.sum(class_probs * np.log2(class_probs))       #expression for entropy -\\sigma p(x)log[p(x)]\n",
    "\n",
    "#computes the gini index cost\n",
    "def cost_gini_index(labels_in_leaf, classes):\n",
    "    class_probs = leaf_count(labels_in_leaf.astype(int), classes) / len(labels_in_leaf)\n",
    "    return 1 - np.sum(np.square(class_probs))               #expression for gini index 1-\\sigma p(x)^2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ebf54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node: \n",
    "    def __init__(self, data_indices, parent):\n",
    "        \"Self is the dt, data_indices should be a numpy array, and parent is a node\"\n",
    "        self.data_indices = data_indices                    #stores the data indices which are in the region defined by this node\n",
    "        self.left = None                                    #stores the left child of the node \n",
    "        self.right = None                                   #stores the right child of the node\n",
    "        self.split_feature = None                           #the feature for split at this node\n",
    "        # ^ the feature will be the string which is the column name\n",
    "        self.split_value = None                             #the value of the feature for split at this node\n",
    "        self.split_cost = None\n",
    "        \n",
    "        if parent:\n",
    "            self.depth = parent.depth + 1                   #obtain the dept of the node by adding one to dept of the parent \n",
    "            self.num_classes = parent.num_classes           #copies the num classes from the parent \n",
    "            self.data = parent.data                         #copies the data from the parent\n",
    "            # the data and labels is in the form of a np array\n",
    "            self.labels = parent.labels                     #copies the labels from the parent\n",
    "            class_prob = leaf_count(self.labels[data_indices], np.unique(self.labels)) #this is counting frequency of different labels in the region defined by this node\n",
    "            self.class_prob = class_prob / np.sum(class_prob)  #stores the class probability for the node\n",
    "            #note that we'll use the class probabilites of the leaf nodes for making predictions after the tree is built\n",
    "            self.binned_labels = class_prob\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b4c0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_test(node, cost_fn, classes=[0, 1]):\n",
    "    \"Takes in a node and cost function. Must work with the np array\"\n",
    "    best_cost = np.inf                              # initialize the best parameter values\n",
    "    best_feature, best_value = None, None           # the best feature is going to be a string, the name of the column. \n",
    "    num_instances, num_features = node.data.shape   # Info on the data subset we split at this node\n",
    "    data_sorted = np.sort(node.data[node.data_indices],axis=0)  # sort the data for this node\n",
    "\n",
    "    for f in range(num_features):   # go through all the features of the data\n",
    "        # stores the data corresponding to the f-th feature\n",
    "        # this is all the VALUES of that feature, over all data at the node\n",
    "        data_f = node.data[node.data_indices, f]    \n",
    "\n",
    "        # we added this next line\n",
    "        test_candidates = np.unique(data_f)    # get all the unique data points. We will try the splits on these values\n",
    "        #for test in test_candidates[:,f]: # test is the test value\n",
    "        for test in test_candidates:\n",
    "            # Split the indices using the test value of f-th feature\n",
    "            # use ~np.isnan(data_f) to exclude indices with nan\n",
    "            left_indices = node.data_indices[np.array(list(data_f <= test) or list(~np.isnan(data_f)))]    # when this feature is less/equal to the test, it would go on left\n",
    "            right_indices = node.data_indices[np.array(list(data_f > test) or list(~np.isnan(data_f)))]    # when the feature is greater than test, it would go on the right\n",
    "            \n",
    "            #we can't have a split where a child has zero element\n",
    "            #if this is true over all the test features and their test values then the function returns the best cost as infinity\n",
    "            if len(left_indices) == 0 or len(right_indices) == 0:                \n",
    "                continue\n",
    "            \n",
    "            #compute the left and right cost based on the current split\n",
    "            left_cost = cost_fn(node.labels[left_indices], classes)\n",
    "            right_cost = cost_fn(node.labels[right_indices], classes)\n",
    "            num_left, num_right = left_indices.shape[0], right_indices.shape[0]\n",
    "            #get the combined cost using the weighted sum of left and right cost\n",
    "            cost = (num_left * left_cost + num_right * right_cost)/num_instances\n",
    "            \n",
    "            #update only when a lower cost is encountered\n",
    "            if cost < best_cost:\n",
    "                best_cost = cost\n",
    "                best_feature = f\n",
    "                best_value = test\n",
    "    return best_cost, best_feature, best_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af665061",
   "metadata": {},
   "source": [
    "## Decision Tree Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e68c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    def __init__(self, num_classes=None, max_depth=3, cost_fn=cost_misclassification, min_leaf_instances=1):\n",
    "        self.max_depth = max_depth      #maximum dept for termination \n",
    "        self.root = None                #stores the root of the decision tree \n",
    "        self.cost_fn = cost_fn          #stores the cost function of the decision tree \n",
    "        self.num_classes = num_classes  #stores the total number of classes\n",
    "        self.min_leaf_instances = min_leaf_instances  #minimum number of instances in a leaf for termination\n",
    "        \n",
    "    def fit(self, data, labels):\n",
    "        \"\"\"\n",
    "        Input \n",
    "            data    the training data features\n",
    "            labels  the training data labels\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.classes = list(np.unique(labels))\n",
    "\n",
    "        if self.num_classes is None:\n",
    "            self.num_classes = len(self.classes)   \n",
    "        self.root = Node(np.arange(data.shape[0]), None)\n",
    "        self.root.data = data\n",
    "        self.root.labels = labels\n",
    "        self.root.num_classes = self.num_classes\n",
    "\n",
    "        self.root.depth = 0\n",
    "        #to recursively build the rest of the tree\n",
    "        self._fit_tree(self.root)\n",
    "        return self\n",
    "\n",
    "    def _fit_tree(self, node):\n",
    "        #This gives the condition for termination of the recursion resulting in a leaf node\n",
    "        if node.depth == self.max_depth or len(node.data_indices) <= self.min_leaf_instances:\n",
    "            return\n",
    "        #greedily select the best test by minimizing the cost\n",
    "        cost, split_feature, split_value = greedy_test(node, self.cost_fn, np.unique(self.labels))\n",
    "        #if the cost returned is infinity it means that it is not possible to split the node and hence terminate\n",
    "        if np.isinf(cost):\n",
    "            return\n",
    "        #to get a boolean array suggesting which data indices corresponding to this node are in the left of the split\n",
    "        test = node.data[node.data_indices,split_feature] <= split_value\n",
    "\n",
    "        # we also now store want the cost to be recorded\n",
    "        node.split_cost = cost\n",
    "\n",
    "        #store the split feature and value of the node\n",
    "        node.split_feature = split_feature\n",
    "        node.split_value = split_value\n",
    "        #define new nodes which are going to be the left and right child of the present node\n",
    "        left = Node(node.data_indices[test], node)\n",
    "        right = Node(node.data_indices[np.logical_not(test)], node)\n",
    "        #recursive call to the _fit_tree()\n",
    "        self._fit_tree(left)\n",
    "        self._fit_tree(right)\n",
    "        #assign the left and right child to present child\n",
    "        node.left = left\n",
    "        node.right = right\n",
    "\n",
    "    # we modified this bit of code so that it can return either the actual class prediction\n",
    "    # or the array of the probabilities of each class (this is the prob boolean input)\n",
    "    def predict(self, data_test: np.array, prob: bool = False):\n",
    "        \"\"\" Prediction function:  \n",
    "        Input: \n",
    "            test_data:   np.array, Data we want to predict its class.\n",
    "            prob:        bool, if True return class probilities else a vector of predictions\n",
    "        \n",
    "        Output:          class probilities else a vector of predictions. \"\"\"\n",
    "\n",
    "        class_probs = np.zeros((data_test.shape[0], self.num_classes, ))\n",
    "        for n, x in enumerate(data_test):   # we need to go through all of the test points, with ... what is n and what is x?\n",
    "            node = self.root\n",
    "\n",
    "            #loop along the depth of the tree looking region where the present data sample fall in based on the split feature and value\n",
    "            while node.left:\n",
    "                if x[node.split_feature] <= int(node.split_value):\n",
    "                    node = node.left\n",
    "                else:\n",
    "                    node = node.right\n",
    "            #the loop terminates when you reach a leaf of the tree and the class probability of that node is taken for prediction\n",
    "\n",
    "            class_probs[n,:] = node.class_prob\n",
    "        if prob:\n",
    "            return class_probs\n",
    "        if self.classes == [0,1]: #work with [2,1] and [1,0] labels\n",
    "            return np.argmax(class_probs,1)\n",
    "        else:\n",
    "            return np.argmax(class_probs,1)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23ef7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best depth for the decision tree\n",
    "# We added this\n",
    "class dtVars:\n",
    "    def __init__(self, f = None, md = None, cf = None, mli = None, pa = None):\n",
    "        self.func = f\n",
    "        self.max_depth = md\n",
    "        self.cost_fn = cf\n",
    "        self.min_leaf_instances = mli\n",
    "        self.pred_ac = pa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c222eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_set(range_of_depths, train_data, validation_data, val_data_y, labels, validate_functions = [] , validate_min_leaf = False):\n",
    "    \"\"\"\n",
    "    Input\n",
    "        range_of_depths     numpy array of the range of depths we will be using to find the best\n",
    "        train_data          the data we will be using to train the possible models, as np.array\n",
    "        validation_data     the data we will use to validate the hyperparams, as np.array\n",
    "        validate_functions  if non empty, we validate which function is best\n",
    "        validate_min_leaf   if non empty, we will validate which min_leaf_instances value is the best\n",
    "    Output\n",
    "        best_variable_set   the set of variables that give highest accuracy on validation set\n",
    "    \"\"\"\n",
    "    variable_funcs = (len(validate_functions) != 0)\n",
    "    variable_leaves = (len(validate_min_leaf) != 0)\n",
    "\n",
    "    best_variable_set = dtVars()    # this will hold the \n",
    "    best_variable_set.pred_ac = -1  # baseline (we will update this as we find better variable setups)\n",
    "\n",
    "    for d in range_of_depths:   # cycle through all depths\n",
    "\n",
    "        if (variable_funcs):\n",
    "            # choose a random function to test\n",
    "            f = validate_functions[random.randint(0,len(validate_functions)-1)]\n",
    "        else:\n",
    "            f = cost_gini_index\n",
    "\n",
    "        if (variable_leaves): \n",
    "            # choose a random leaf value to test\n",
    "            l = validate_min_leaf[random.randint(0,len(validate_min_leaf)-1)]\n",
    "        else:\n",
    "            l = 1\n",
    "\n",
    "        # make dt instance\n",
    "        dt = DecisionTree(num_classes=2, max_depth=d, cost_fn = f, min_leaf_instances=l)\n",
    "\n",
    "        # train the dt\n",
    "        dt.fit(train_data, labels)  # the labels are th y values associated with train_data features\n",
    "\n",
    "        # validate this model bu predicting labels of validation_data\n",
    "        preds = dt.predict(validation_data)\n",
    "\n",
    "        pred_accuracy = evaluate_acc(preds, val_data_y)\n",
    "\n",
    "        # if the prediction accuracy of this setup is higher than existing then we update the best_variable_set\n",
    "        if (pred_accuracy > best_variable_set.pred_ac):\n",
    "            best_variable_set.pred_ac = pred_accuracy\n",
    "            best_variable_set.max_depth = d\n",
    "            best_variable_set.cost_fn = f\n",
    "            best_variable_set.min_leaf_instances = l\n",
    "\n",
    "    return best_variable_set    # this will now hold the absolute best of the variable sets we saw\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1dfe1b",
   "metadata": {},
   "source": [
    "# Experimenting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd15669",
   "metadata": {},
   "source": [
    "## Training utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c682fb5",
   "metadata": {},
   "source": [
    "### Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7188d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def KFold(n_splits, X_input, shuffle=True):\n",
    "    # not exactly as the KFold sklearn fucntion but should do the job \n",
    "    \"\"\"\n",
    "    Input\n",
    "        n_splits:   number of splits of teh data\n",
    "        X_input:    data\n",
    "        shuffle:    if we need to shuffle the data or not\n",
    "    Return\n",
    "        test_indexs:    array of length n_splits, where at each spot there is a subarray containing a set of test indices\n",
    "                        ex. test_index[i] contains the test set of ith split\n",
    "        train_indexs:   array of length n_splits, where at each spot there is a subarray containing a set of train indices\n",
    "    \"\"\"\n",
    "    index_list=[i for i in range(len(X_input))]\n",
    "    \n",
    "    if shuffle: # Shuffle the indices if they need to be shuffled\n",
    "        random.shuffle(index_list)      \n",
    "    \n",
    "    length=int(len(X_input)/n_splits)   # The length of each block is the input_length / number of splits\n",
    "\n",
    "    test_indexs, train_indexs = [], []  # This will be the lists of the blocks\n",
    "    \n",
    "    for i in range(n_splits):           # Split the data into test and train based on our shuffled index\n",
    "        test_index = index_list[i*length:(i+1)*length]  # the indices counting to the first test block\n",
    "        test_indexs.append(test_index)                  # add this list of indices to the test block.                             \n",
    "        train_indexs.append(list(set(index_list) - set(test_index)))    # the rest of the indices are the training block\n",
    "\n",
    "    return test_indexs, train_indexs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dab4dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate(model, X, Y, n_splits=5, standarize=False, name=None, columns_index=None, prob=False):\n",
    "    \"\"\"\n",
    "    Input\n",
    "        model:  The model we want to validate\n",
    "        X:      The features from the data we are working with\n",
    "        Y:      The labels of the data we are working with\n",
    "    Return\n",
    "        y:      Array of length n_split, which at the ith position contains a subarray of the actual data labels\n",
    "        yh:     Array of length n_split, which at the ith position contains a subarray of predicted labels for the ith test set in split\n",
    "    \"\"\"\n",
    "    test_indexs, train_indexs = KFold(n_splits, X, shuffle=True) # split the data\n",
    "\n",
    "    y = np.array([0] * X.shape[0])\n",
    "    if prob:\n",
    "        y = []\n",
    "        yh = []\n",
    "    else:\n",
    "        yh = np.array([0] * X.shape[0])\n",
    "    for train_index, test_index in zip(train_indexs, test_indexs):  # for each split \n",
    "        X_train=X[train_index]\n",
    "        X_test=X[test_index]\n",
    "        if standarize:\n",
    "            X_train=Standarize(X_train, name, columns_index)\n",
    "            X_test=Standarize(X_test, name, columns_index)\n",
    "\n",
    "        model.fit(X_train, Y[train_index])                   # fit the current training data (of cur. split bunch)\n",
    "        if prob:\n",
    "            yt= model.predict(X_test, prob=prob)               # store the predicted values\n",
    "            y.append(Y[test_index])\n",
    "            yh.append(yt)\n",
    "        else:\n",
    "            y[test_index] = Y[test_index]                               # get the y values of the data currently acting as test\n",
    "            yh[test_index] = model.predict(X_test)\n",
    "    if prob:\n",
    "        return np.array(y).reshape(len(yt)*n_splits), np.array(yh).reshape(len(yt)*n_splits, 2)\n",
    "    return y, yh    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ede847",
   "metadata": {},
   "source": [
    "### Test Train Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c6e144",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_train_split(X_input, Y, proportions=[4, 1], shuffle=True, standarize=False, name=None, columns_index=None):\n",
    "    \"\"\"\n",
    "    Input\n",
    "        X_input:     The features from the data we are working with\n",
    "        Y:           The labels of the data we are working with\n",
    "        proportions: Train/Test proportions \n",
    "    Return\n",
    "        X_train:     Data that will be used to fit/train the model (80% of X)\n",
    "        y_train:     X_train labels\n",
    "        X_test:      Data that will be used to test the model (20% of X)\n",
    "        y_test:      X_test labels\n",
    "    \"\"\"\n",
    "    index_list=[i for i in range(len(X_input))]\n",
    "    \n",
    "    if shuffle: # Shuffle the indices if they need to be shuffled\n",
    "        random.shuffle(index_list)      \n",
    "    length=len(index_list) # get length of data\n",
    "    train_length=int(length*proportions[0]/(proportions[0]+proportions[1])) # get length of data that will be used as train\n",
    "    # test_length is the rest\n",
    "    X_train, y_train=X_input[index_list[:train_length]], Y[index_list[:train_length]]\n",
    "    X_test, y_test=X_input[index_list[train_length:]], Y[index_list[train_length:]]\n",
    "    if standarize:\n",
    "        X_train=Standarize(X_train, name, columns_index)\n",
    "        X_test=Standarize(X_test, name, columns_index)\n",
    "    return X_train, y_train, X_test, y_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0ffd84",
   "metadata": {},
   "source": [
    "### Model Accuracy Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d5ff33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_accuracy(X, y, model, cv = True, n_splits=5, proportions=[4, 1], prints=True, shuffle=False, standarize=False, name=None, columns_index=None):\n",
    "    X_train, y_train, X_test, y_test = test_train_split(X, y, proportions=proportions, shuffle=shuffle, standarize=standarize, name=name, columns_index=columns_index)\n",
    "    model.fit(X_train,y_train)\n",
    "    y_preds = model.predict(X_test)\n",
    "    test_acc=evaluate_acc(y_preds, y_test)*100\n",
    "    if prints:\n",
    "        print(\"Accuracy on test set: \" + str(test_acc)+\"%\")\n",
    "    \n",
    "    if (cv):\n",
    "        y_preds, yh = cross_validate(model, X, y, n_splits=n_splits)\n",
    "        crossval_acc=evaluate_acc(y_preds, yh)*100\n",
    "        if prints:\n",
    "            print(\"Accuracy using Cross Validation: \" + str(crossval_acc)+\"%\")\n",
    "        return test_acc, crossval_acc\n",
    "        \n",
    "    return test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686510fc",
   "metadata": {},
   "source": [
    "## 1. Compare the accuracy of KNN and DT algorithm on the two datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80dc0679",
   "metadata": {},
   "source": [
    "### hepatitis data: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed707191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hepatitis_cleaned data: df_hepatitis_cleaned\n",
    "numpy_X_hep_cleaned = df_hepatitis_cleaned.drop(\"Class\", axis=1).to_numpy()\n",
    "numpy_y_hep_cleaned = df_hepatitis_cleaned.Class.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f19890e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test kNN with k=2 and euclidian distance \n",
    "f = euclidean\n",
    "print(\"hepatitis data: kNN with 2 neighbors and the euclidian dist:\")\n",
    "get_model_accuracy(numpy_X_hep_cleaned, numpy_y_hep_cleaned, kNN(2, f, function_type(f)))\n",
    "\n",
    "print()\n",
    "\n",
    "# now lets test with weighted kNN\n",
    "print(\"hepatitis data: weighted kNN with 2 neighbors and the euclidian dist\")\n",
    "get_model_accuracy(numpy_X_hep_cleaned, numpy_y_hep_cleaned, kNN(2, f, function_type(f), True))\n",
    "\n",
    "print()\n",
    "\n",
    "#test dt with 2 min leafs and max depth equal to 3 and cost_gini_index \n",
    "print(\"hepatitis data: Decision Tree with max depth equal to 3 and min leafs equal to 2\")\n",
    "get_model_accuracy(numpy_X_hep_cleaned, numpy_y_hep_cleaned, DecisionTree(2,3,cost_gini_index,2))\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576770ff",
   "metadata": {},
   "source": [
    "### messidor data: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba88cfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hepatitis data: df_messidor_features_cleaned\n",
    "numpy_y_mess_cleaned = df_messidor_features_cleaned.Class.to_numpy()\n",
    "numpy_X_mess_cleaned = df_messidor_features_cleaned.drop(\"Class\", axis=1).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcba23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test kNN with k=2 and euclidian distance \n",
    "f = euclidean\n",
    "print(\"messidor data: kNN with 2 neighbors and the euclidian dist:\")\n",
    "get_model_accuracy(numpy_X_mess_cleaned, numpy_y_mess_cleaned, kNN(2, f, function_type(f)))\n",
    "\n",
    "print()\n",
    "\n",
    "# now lets test with weighted kNN\n",
    "print(\"messidor data: weighted kNN with 2 neighbors and the euclidian dist\")\n",
    "get_model_accuracy(numpy_X_mess_cleaned, numpy_y_mess_cleaned, kNN(2, f, function_type(f), True))\n",
    "\n",
    "print()\n",
    "\n",
    "#test dt with 2 min leafs and max depth equal to 3 and cost_gini_index \n",
    "print(\"messidor data: Decision Tree with max depth equal to 3 and min leafs equal to 2\")\n",
    "get_model_accuracy(numpy_X_mess_cleaned, numpy_y_mess_cleaned, DecisionTree(2,3,cost_gini_index,2), shuffle=True)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe28b71b",
   "metadata": {},
   "source": [
    "### Now lets test on normalized data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e129f77",
   "metadata": {},
   "source": [
    "#### hepatitis data: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82a6ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test kNN with k=2 and euclidian distance \n",
    "f = euclidean\n",
    "print(\"hepatitis data: kNN with 2 neighbors and the euclidian dist:\")\n",
    "get_model_accuracy(numpy_X_hep_cleaned, numpy_y_hep_cleaned, kNN(2, f, function_type(f)), standarize=True, name=\"hepatitis\")\n",
    "\n",
    "print()\n",
    "\n",
    "# now lets test with weighted kNN\n",
    "print(\"hepatitis data: weighted kNN with 2 neighbors and the euclidian dist\")\n",
    "get_model_accuracy(numpy_X_hep_cleaned, numpy_y_hep_cleaned, kNN(2, f, function_type(f), True), standarize=True, name=\"hepatitis\")\n",
    "\n",
    "print()\n",
    "\n",
    "#test dt with 2 min leafs and max depth equal to 3 and cost_gini_index \n",
    "print(\"hepatitis data: Decision Tree with max depth equal to 3 and min leafs equal to 2\")\n",
    "get_model_accuracy(numpy_X_hep_cleaned, numpy_y_hep_cleaned, DecisionTree(2,3,cost_gini_index,2), standarize=True, name=\"hepatitis\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d685ea5",
   "metadata": {},
   "source": [
    "### messidor data: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005cabb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test kNN with k=2 and euclidian distance \n",
    "f = euclidean\n",
    "print(\"messidor data: kNN with 2 neighbors and the euclidian dist:\")\n",
    "get_model_accuracy(numpy_X_mess_cleaned, numpy_y_mess_cleaned, kNN(2, f, function_type(f)),standarize=True, name=\"messidor\")\n",
    "\n",
    "print()\n",
    "\n",
    "# now lets test with weighted kNN\n",
    "print(\"messidor data: weighted kNN with 2 neighbors and the euclidian dist\")\n",
    "get_model_accuracy(numpy_X_mess_cleaned, numpy_y_mess_cleaned, kNN(2, f, function_type(f), True),standarize=True, name=\"messidor\")\n",
    "\n",
    "print()\n",
    "\n",
    "#test dt with 2 min leafs and max depth equal to 3 and cost_gini_index \n",
    "print(\"messidor data: Decision Tree with max depth equal to 3 and min leafs equal to 2\")\n",
    "get_model_accuracy(numpy_X_mess_cleaned, numpy_y_mess_cleaned, DecisionTree(2,3,cost_gini_index,2), shuffle=True,standarize=True, name=\"messidor\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e422abc",
   "metadata": {},
   "source": [
    "## 2. Test different K values and see how it affects the training data accuracy and test data accuracy of KNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8610e348",
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbors_to_test = [k for k in range(1,11)]\n",
    "performance_hep = [0] * 10               # init array we will store performance in\n",
    "performance_messi = [0] * 10  \n",
    "performance_hep_crossval = [0] * 10               # init array we will store performance in\n",
    "performance_messi_crossval = [0] * 10  \n",
    "\n",
    "f = euclidean \n",
    "for i in range(len(neighbors_to_test)):\n",
    "    knn = kNN(neighbors_to_test[i], f, function_type(f), True)\n",
    "    performance_messi[i], performance_messi_crossval[i] = get_model_accuracy(numpy_X_mess_cleaned, numpy_y_mess_cleaned, knn, prints=False, standarize=True, name=\"messidor\")\n",
    "    performance_hep[i], performance_hep_crossval[i] = get_model_accuracy(numpy_X_hep_cleaned, numpy_y_hep_cleaned, knn, prints=False, standarize=True, name=\"hepatitis\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cadd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "ax.plot(neighbors_to_test, performance_hep, label='hepatitis')\n",
    "ax.plot(neighbors_to_test, performance_hep_crossval, label='hepatitis crossval')\n",
    "ax.plot(neighbors_to_test, performance_messi, label='messidor')\n",
    "ax.plot(neighbors_to_test, performance_messi_crossval, label='messidor crossval')\n",
    "\n",
    "plt.xticks(neighbors_to_test)\n",
    "plt.ylim(50,100)\n",
    "ax.legend()\n",
    "plt.xlabel('N of neighbors')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy of kNN on both datasets for different number of neighbors')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b2f821",
   "metadata": {},
   "source": [
    "## 3. Similarly, check how maximum tree depth can affect the performance of DT on the provided datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d5e951",
   "metadata": {},
   "outputs": [],
   "source": [
    "depths_to_test = [k for k in range(1,16)]\n",
    "performance_hep = [0] * 15               # init array we will store performance in\n",
    "performance_messi = [0] * 15  \n",
    "performance_hep_crossval = [0] * 15               # init array we will store performance in\n",
    "performance_messi_crossval = [0] * 15  \n",
    "\n",
    "f = cost_gini_index\n",
    "l = 2\n",
    "for i in range(len(depths_to_test)):\n",
    "    dt=DecisionTree(num_classes = 2, max_depth = depths_to_test[i], cost_fn = f, min_leaf_instances=l) \n",
    "    performance_messi[i], performance_messi_crossval[i] = get_model_accuracy(numpy_X_mess_cleaned, numpy_y_mess_cleaned, dt, prints=False, standarize=True, name=\"messidor\")\n",
    "    performance_hep[i], performance_hep_crossval[i] = get_model_accuracy(numpy_X_hep_cleaned, numpy_y_hep_cleaned, dt, prints=False, standarize=True, name=\"hepatitis\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a67359d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "ax.plot(depths_to_test, performance_hep, label='hepatitis')\n",
    "ax.plot(depths_to_test, performance_hep_crossval, label='hepatitis cross val')\n",
    "ax.plot(depths_to_test, performance_messi, label='messidor')\n",
    "ax.plot(depths_to_test, performance_messi_crossval, label='messidor cross val')\n",
    "\n",
    "plt.xticks(depths_to_test)\n",
    "plt.ylim(50,100)\n",
    "ax.legend()\n",
    "plt.xlabel('N of depths')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy of dt on both datasets for different number of depths')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d573fbf4",
   "metadata": {},
   "source": [
    "## 4. Try out different distance/cost functions for both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb054a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_functions = [cost_misclassification, cost_entropy, cost_gini_index]\n",
    "performance_hep = [0] * 3               # init array we will store performance in\n",
    "performance_messi = [0] * 3  \n",
    "performance_hep_crossval = [0] * 3               # init array we will store performance in\n",
    "performance_messi_crossval = [0] * 3  \n",
    "\n",
    "l = 2\n",
    "m = 3\n",
    "for i in range(len(cost_functions)):\n",
    "    dt=DecisionTree(num_classes = 2, max_depth = m, cost_fn = cost_functions[i], min_leaf_instances=l) \n",
    "    performance_messi[i], performance_messi_crossval[i] = get_model_accuracy(numpy_X_mess_cleaned, numpy_y_mess_cleaned, dt, prints=False, standarize=True, name=\"messidor\")\n",
    "    performance_hep[i], performance_hep_crossval[i] = get_model_accuracy(numpy_X_hep_cleaned, numpy_y_hep_cleaned, dt, prints=False, standarize=True, name=\"hepatitis\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74a7454",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "ax.scatter(['Misclassification', 'Entropy', 'Gini index'], performance_hep, label='hepatitis')\n",
    "ax.scatter(['Misclassification', 'Entropy', 'Gini index'], performance_hep_crossval, label='hepatitis crossval')\n",
    "ax.scatter(['Misclassification', 'Entropy', 'Gini index'], performance_messi, label='messidor')\n",
    "ax.scatter(['Misclassification', 'Entropy', 'Gini index'], performance_messi_crossval, label='messidor crossval')\n",
    "\n",
    "plt.ylim(50,100)\n",
    "ax.legend()\n",
    "plt.xlabel('cost functions')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy of dt on both datasets for different cost functions')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b56127",
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_functions = [manhattan, euclidean, cosine_similarity]\n",
    "performance_hep = [0] * 3               # init array we will store performance in\n",
    "performance_messi = [0] * 3  \n",
    "performance_hep_crossval = [0] * 3               # init array we will store performance in\n",
    "performance_messi_crossval = [0] * 3  \n",
    "\n",
    "k = 2\n",
    "for i in range(len(cost_functions)):\n",
    "    knn=kNN(k, cost_functions[i], function_type(cost_functions[i]), True)\n",
    "    performance_messi[i], performance_messi_crossval[i] = get_model_accuracy(numpy_X_mess_cleaned, numpy_y_mess_cleaned, knn, prints=False,standarize=True, name=\"messidor\")\n",
    "    performance_hep[i], performance_hep_crossval[i] = get_model_accuracy(numpy_X_hep_cleaned, numpy_y_hep_cleaned, knn, prints=False,standarize=True, name=\"hepatitis\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4ea06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "ax.scatter(['Manhattan', 'Euclidean', 'Cosine Similarity'], performance_hep, label='hepatitis')\n",
    "ax.scatter(['Manhattan', 'Euclidean', 'Cosine Similarity'], performance_hep_crossval, label='hepatitis crossval')\n",
    "ax.scatter(['Manhattan', 'Euclidean', 'Cosine Similarity'], performance_messi, label='messidor')\n",
    "ax.scatter(['Manhattan', 'Euclidean', 'Cosine Similarity'], performance_messi_crossval, label='messidor crossval')\n",
    "\n",
    "plt.ylim(50,100)\n",
    "ax.legend()\n",
    "plt.xlabel('cost functions')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy of kNN on both datasets for different cost functions')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2aa947",
   "metadata": {},
   "source": [
    "## 5. Present two plots of the decision boundaries one for KNN and one for DT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe949264",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_boundaries(model, min_feat_values, max_feat_values, x_train, y_train, feat_names, graph_name):\n",
    "    \"\"\"\n",
    "    Input\n",
    "        model               the model for which we are making decision boundaries\n",
    "        min_feat_values     an np array of 2 min values\n",
    "        max_feat_values     an np array of 2 max values\n",
    "        x_train             training data\n",
    "        y_train             training labels\n",
    "        feat_names          names of two \"best\" features\n",
    "    Output\n",
    "        none. prints knn boundary plot for both data sets\n",
    "    \"\"\"\n",
    "    feat1_values = np.linspace(min_feat_values[0], max_feat_values[0], 200)   # generate values for the number of features we want\n",
    "    feat2_values = np.linspace(min_feat_values[1], max_feat_values[1], 200)\n",
    "\n",
    "    x0,x1 = np.meshgrid(feat1_values, feat2_values)\n",
    "    x_all = np.vstack((x0.ravel(),x1.ravel())).T\n",
    "\n",
    "    # only plot the decision boundaries for one value of k\n",
    "    y_train_prob = np.zeros((y_train.shape[0], len(model.classes))) # make empty array that will hole predicted labels\n",
    "    y_train_prob[np.arange(y_train.shape[0]), y_train - 1] = 1\n",
    "    y_pred = model.predict(x_all)\n",
    "    \n",
    "    plt.clf()\n",
    "    plt.scatter(x_train[:,0], x_train[:,1], c = y_train, marker='o', alpha=1)\n",
    "    plt.scatter(x_all[:,0], x_all[:,1], c = y_pred, marker='.', alpha=.01)\n",
    "    plt.ylabel('feature 1: ' + str(feat_names[0]))\n",
    "    plt.xlabel('feature 2: ' + str(feat_names[1]))\n",
    "    plt.title(graph_name)\n",
    "    plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4078c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=np.array(df_hepatitis_cleaned.Class)\n",
    "X=np.array(df_hepatitis_cleaned.drop(\"Class\", axis=1))\n",
    "X_train_hepatitis_normalized, y_train_hepatitis_normalized, X_test_hepatitis_normalized, y_test_hepatitis_normalized = test_train_split(X, y, standarize=True, name=\"hepatitis\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84e3680",
   "metadata": {},
   "source": [
    "Features used are spiders and protime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf940a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = kNN(2, euclidean, function_type(euclidean), False)\n",
    "min_feat_values = [np.min(X_train_hepatitis_normalized[:,0]), np.min(X_train_hepatitis_normalized[:,16])]\n",
    "max_feat_values = [np.max(X_train_hepatitis_normalized[:,0]), np.max(X_train_hepatitis_normalized[:,16])]\n",
    "X_boundary = X_train_hepatitis_normalized[:,[0,16]]\n",
    "knn.fit(X_boundary, y_train_hepatitis_normalized)\n",
    "decision_boundaries(knn, min_feat_values, max_feat_values, X_boundary, y_train_hepatitis_normalized, [10,17], \"Hepatitis decision boundaries, knn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82df15c",
   "metadata": {},
   "source": [
    "Features used are varices and ascites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a74514",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTree(2, 3,cost_entropy,2)\n",
    "min_feat_values = [np.min(X_train_hepatitis_normalized[:,0]), np.min(X_train_hepatitis_normalized[:,16])]\n",
    "max_feat_values = [np.max(X_train_hepatitis_normalized[:,0]), np.max(X_train_hepatitis_normalized[:,16])]\n",
    "X_boundary = X_train_hepatitis_normalized[:,[0, 16]]\n",
    "dt.fit(X_boundary, y_train_hepatitis_normalized)\n",
    "decision_boundaries(dt, min_feat_values, max_feat_values, X_boundary, y_train_hepatitis_normalized, [0,16], \"Hepatitis decision boundaries, dt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7fb09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=np.array(df_messidor_features_cleaned.Class)\n",
    "X=np.array(df_messidor_features_cleaned.drop(\"Class\", axis=1))\n",
    "X_train_messidor_normalized, y_train_messidor_normalized, X_test_messidor_normalized, y_test_messidor_normalized = test_train_split(X, y, standarize=True, name=\"messidor\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45c01c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = kNN(2, euclidean, function_type(euclidean), False)\n",
    "min_feat_values = [np.min(X_train_messidor_normalized[:,2]), np.min(X_train_messidor_normalized[:,3])]\n",
    "max_feat_values = [np.max(X_train_messidor_normalized[:,2]), np.max(X_train_messidor_normalized[:,3])]\n",
    "X_boundary = X_train_messidor_normalized[:,[2,3]]\n",
    "knn.fit(X_boundary, y_train_messidor_normalized)\n",
    "decision_boundaries(knn, min_feat_values, max_feat_values, X_boundary, y_train_messidor_normalized, [2,3], \"messidor decision boundaries, knn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2723b46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTree(2, 3,cost_entropy,2)\n",
    "min_feat_values = [np.min(X_train_messidor_normalized[:,2]), np.min(X_train_messidor_normalized[:,3])]\n",
    "max_feat_values = [np.max(X_train_messidor_normalized[:,2]), np.max(X_train_messidor_normalized[:,3])]\n",
    "X_boundary = X_train_messidor_normalized[:,[2,3]]\n",
    "dt.fit(X_boundary, y_train_messidor_normalized)\n",
    "decision_boundaries(dt, min_feat_values, max_feat_values, X_boundary, y_train_messidor_normalized, [2,3], \"messidor decision boundaries, dt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfd5ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def viz_dt(dt, supress = False):\n",
    "    \"\"\" \n",
    "    # maybe have a counetr \n",
    "    Input\n",
    "        dt  the decision tree to visualize\n",
    "    Output\n",
    "        feats   returns features used by the dt\n",
    "    \"\"\"\n",
    "    space = \"          \"    # i will have to think on this more\n",
    "    q = [dt.root]\n",
    "    feats = []\n",
    "    \n",
    "\n",
    "    while len(q) != 0:  # while the q is not empty\n",
    "        n = q.pop()     # pop a node from the queue\n",
    "        s = \"\"\n",
    "        if (n != dt.root):  # print the non-root nodes\n",
    "            if (n.split_feature):\n",
    "                s = s + str(n.split_feature) + '<=' + str(n.split_value) + \" & \"\n",
    "                feats.append(n.split_feature)\n",
    "            if(not supress):\n",
    "                s = s + \"N = \" + str(len(n.data_indices)) + \" & \"\n",
    "                s = s + str(n.binned_labels)\n",
    "        else:               # print the root node\n",
    "            if (n.split_feature):\n",
    "                s = s + str(n.split_feature) + '<=' + str(n.split_value) + \" & \"\n",
    "                feats.append(n.split_feature)\n",
    "            if(not supress):\n",
    "                s = s + \"N = \" + str(len(n.data_indices)) + \" & \"\n",
    "                s = s + str(leaf_count(dt.labels, np.unique(dt.labels)))\n",
    "\n",
    "        # if the node has children, append them to the list q\n",
    "        if n.left:\n",
    "            q.append(n.left)\n",
    "        if n.right:\n",
    "            q.append(n.right)\n",
    "        if(not supress):\n",
    "            print(s)\n",
    "    return feats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799e4d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the tree\n",
    "y=np.array(df_hepatitis_cleaned.Class)\n",
    "X=np.array(df_hepatitis_cleaned.drop(\"Class\", axis=1))\n",
    "X_train_hepatitis_cleaned, y_train_hepatitis_cleaned, X_test_hepatitis_cleaned, y_test_hepatitis_cleaned = test_train_split(X, y)\n",
    "\n",
    "dt = DecisionTree(num_classes = 2, max_depth = 3, cost_fn = cost_gini_index, min_leaf_instances=1)\n",
    "dt.fit(X_train_hepatitis_cleaned, y_train_hepatitis_cleaned)\n",
    "\n",
    "#dt.root.labels\n",
    "hep_feats = viz_dt(dt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2cb7a86",
   "metadata": {},
   "source": [
    "## Feature Selection "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4126f476",
   "metadata": {},
   "source": [
    "### method 1: selecting correlated features with the target "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1fe334",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection(n, dataset, labels):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        n:              number of features to return\n",
    "        dataset:        np.array that contains all the features (in all other columns)\n",
    "        labels:         np.array the labels \n",
    "    Return:\n",
    "        top_n_features: an np.array of indices which indicate the features that have highest correlation with the labels\n",
    "                        note that we are basing this off the absolute value, since both very positive and\n",
    "                        very negative correlation coefficients are informative\n",
    "    \"\"\"\n",
    "    \n",
    "    total_columns = dataset.shape[1]\n",
    "    top_n_features = np.zeros((total_columns, 2)) # we have n rows, with column 0 having feature value, column 2 holding abs cor coef\n",
    "\n",
    "    for feature in range(total_columns):     # we start at column 1, because col 0 is the label\n",
    "        # for each feature, we correlate labels and features\n",
    "        feature_values = dataset[:,feature]\n",
    "        r = np.corrcoef(labels, feature_values)\n",
    "        val = r[0][1]\n",
    "        top_n_features[feature-1][0] = int(feature)     # add feature value to the first element\n",
    "        top_n_features[feature-1][1] = abs(val)         # add correlation coefficient to the features list\n",
    "        # we do feature-1, since featrues start at index 1\n",
    "\n",
    "    top_n_features = top_n_features[np.argsort(-top_n_features[:, 1])] # sort by cor coef values\n",
    "\n",
    "    return top_n_features[0:n]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84be7b5",
   "metadata": {},
   "source": [
    "#### Hepatitis Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876be88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hep_features = feature_selection(10, numpy_X_hep_cleaned, numpy_y_hep_cleaned)\n",
    "print(\"Top features:\")\n",
    "print(hep_features[:,0].astype(int))\n",
    "print()\n",
    "print(\"Top features names:\")\n",
    "print(df_hepatitis_cleaned.columns[hep_features[:,0].astype(int)+1]) \n",
    "# +1 because the first column is class and was not used to get corr\n",
    "print()\n",
    "print(\"Their importance:\")\n",
    "print(hep_features[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61efb0c6",
   "metadata": {},
   "source": [
    "#### Messidor Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fd9f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "mess_features = feature_selection(10, numpy_X_mess_cleaned, numpy_y_mess_cleaned)\n",
    "print(\"Top features:\")\n",
    "print(mess_features[:,0].astype(int))\n",
    "print()\n",
    "print(\"Top features names:\")\n",
    "print(df_messidor_features_cleaned.columns[mess_features[:,0].astype(int)]) \n",
    "# +1 because the first column is class and was not used to get corr\n",
    "print()\n",
    "print(\"Their importance:\")\n",
    "print(mess_features[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ecd354",
   "metadata": {},
   "source": [
    "#### Reduced Numpy Arrays\n",
    "These will contain only the features with highest correlation coeficient. For the messidor data, we have one with the golbal top three features, and one with the top three features with unique correlation coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3eca23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy_hep=numpy_X_hep_cleaned[:, [10, 15, 17,  16]]\n",
    "numpy_mess = numpy_X_mess_cleaned[:, [2, 3, 4, 5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f613f32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will normalize only features 15,17,16 for hep data because 10 is 1/0 (categ) so the input of stand function \n",
    "# will become the index of these 3 features in numpy_hep \n",
    "# all features of numpy_mess will be normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb43774f",
   "metadata": {},
   "source": [
    "### method 2: exhaustive_feature_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47dc9411",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exhaustive_feature_selection(dataset, labels, model, n_splits=5):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        dataset:         np.array that contains all the features (in all other columns)\n",
    "        labels:          np.array the labels \n",
    "        model:           model to test\n",
    "        n_splits:        n splits used in cross val \n",
    "    Return:\n",
    "        top_n_features:  an np.array of indices which indicate the order of features that got selected one by one.\n",
    "        best_accuracies: Each accuracy was generated used the n-top features combined\n",
    "    \"\"\"\n",
    "        \n",
    "    total_columns = dataset.shape[1]\n",
    "    top_n_features = [] # we have n rows, with column 0 having feature value, column 2 holding abs cor coef\n",
    "\n",
    "    selected_data=[]\n",
    "    best_accuracies=[] \n",
    "\n",
    "    for feature in range(total_columns):    # we start at column 1, because col 0 is the label\n",
    "        accuracies=[]                       # we have an \"feature\" number of accuracy arrays\n",
    "\n",
    "        for n in range(total_columns):      # for all columns\n",
    "            \n",
    "            if n in top_n_features:         # if the feature is already in top_n_features\n",
    "                accuracies.append(-1)       # append -1 to the list and go to next feature\n",
    "                continue                    # this is so that we dont count the features twice\n",
    "\n",
    "            # make copies of the data, because we will be making some modifications in order to test further accuracy\n",
    "            selected_data_copy= selected_data.copy()\n",
    "            \n",
    "            feature_values = dataset[:,n]                     # get the values of the feature we are looking at now\n",
    "            selected_data_copy.append(list(feature_values))   # add this feature to the data we are training our moedl on\n",
    "            \n",
    "            data_to_test = np.array(selected_data_copy) \n",
    "            data_to_test = data_to_test.transpose(1,0)\n",
    "            y_preds, yh = cross_validate(model, data_to_test, labels, n_splits=n_splits)\n",
    "            # we use cross val to select the best accuracybecause it gives uses the whole data\n",
    "            \n",
    "            # test the model and evaluate accuracy (of using these features)\n",
    "            crossval_acc=evaluate_acc(y_preds, yh)*100\n",
    "            accuracies.append(crossval_acc)\n",
    "\n",
    "        # choose the best feature, and add it to our main training set\n",
    "        best_feature_index=accuracies.index(max(accuracies))\n",
    "\n",
    "        # append the best feature data to our main training set\n",
    "        selected_data.append(dataset[:,best_feature_index])\n",
    "        top_n_features.append(best_feature_index)\n",
    "        best_accuracies.append(max(accuracies))\n",
    "\n",
    "        # the way the above works is that the first feature column is the one that, on its own, predicted\n",
    "        # with the best accuracy.\n",
    "        # the second feature column is what gave us the best accuracy, when combined with column 1, etc.\n",
    "\n",
    "    return top_n_features, best_accuracies            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f352b4",
   "metadata": {},
   "source": [
    "#### DT feature selection using method 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44337a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTree(2,3,cost_misclassification,2)\n",
    "print(\"hep data: DT\")\n",
    "get_model_accuracy(numpy_hep, numpy_y_hep_cleaned, dt, True, standarize=True,columns_index=[1,2,3])\n",
    "print()\n",
    "print(\"mess data: DT\")\n",
    "get_model_accuracy(numpy_mess, numpy_y_mess_cleaned, dt, True, standarize=True,columns_index=[0,1,2,3])\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99d1876",
   "metadata": {},
   "source": [
    "#### DT feature selection using method 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4b5da0",
   "metadata": {},
   "source": [
    "messidor data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f5728c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTree(2, 3,cost_entropy,2)\n",
    "top_n_features, best_accuracies =exhaustive_feature_selection(numpy_X_mess_cleaned, numpy_y_mess_cleaned, dt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a43ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "ax.plot(df_messidor_features_cleaned.drop('Class', axis=1).columns[top_n_features] , best_accuracies)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "# plt.xticks(top_n_features.astype(str))\n",
    "plt.ylim(50,80)\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy was obtained using N and N-1 features combined')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45251ceb",
   "metadata": {},
   "source": [
    "hepatitis data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9ba2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTree(2, 3,cost_entropy,2)\n",
    "top_n_features, best_accuracies =exhaustive_feature_selection(numpy_X_hep_cleaned, numpy_y_hep_cleaned, dt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cafc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "ax.plot(df_hepatitis_cleaned.drop('Class', axis=1).columns[top_n_features] , best_accuracies)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "# plt.xticks(top_n_features.astype(str))\n",
    "plt.ylim(70,100)\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy was obtained using N and N-1 features combined')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e187cddb",
   "metadata": {},
   "source": [
    "The features in hepatitis that were consistently used were 1 and 2\n",
    "The features in messidor that were consistently used were 1 and 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3bb1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = kNN(2, euclidean, function_type(euclidean), False)\n",
    "print(\"hep data: knn\")\n",
    "get_model_accuracy(numpy_hep, numpy_y_hep_cleaned, knn, True, standarize=True, columns_index=[1,2,3])\n",
    "print()\n",
    "print(\"mess data: knn\")\n",
    "get_model_accuracy(numpy_mess, numpy_y_mess_cleaned, knn, True, standarize=True, columns_index=[0, 1,2,3])\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94d12ec",
   "metadata": {},
   "source": [
    "#### knn feature selection using method 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8c4e76",
   "metadata": {},
   "source": [
    "messidor data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d82819b",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = kNN(2, euclidean, function_type(euclidean), False)\n",
    "top_n_features, best_accuracies =exhaustive_feature_selection(numpy_X_mess_cleaned, numpy_y_mess_cleaned, knn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e169de",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "ax.plot(df_messidor_features_cleaned.drop('Class', axis=1).columns[top_n_features] , best_accuracies, label='messidor cross val')\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "# plt.xticks(top_n_features.astype(str))\n",
    "plt.ylim(50,80)\n",
    "ax.legend()\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy was obtained using N and N-1 features combined')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52cfdbf",
   "metadata": {},
   "source": [
    "hepatitis data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ac099e",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = kNN(2, euclidean, function_type(euclidean), False)\n",
    "top_n_features, best_accuracies =exhaustive_feature_selection(numpy_X_hep_cleaned, numpy_y_hep_cleaned, knn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea03ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "ax.plot(df_hepatitis_cleaned.drop('Class', axis=1).columns[top_n_features] , best_accuracies)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "# plt.xticks(top_n_features.astype(str))\n",
    "plt.ylim(70,100)\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy was obtained using N and N-1 features combined')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e1c05a",
   "metadata": {},
   "source": [
    "## HP selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd077f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hepatitis=np.array(df_hepatitis_cleaned.Class)\n",
    "X_hepatitis=np.array(df_hepatitis_cleaned.drop(\"Class\", axis=1))\n",
    "X_train_hepatitis_normalized, y_train_hepatitis_normalized, X_test_hepatitis_normalized, y_test_hepatitis_normalized = test_train_split(X_hepatitis, y_hepatitis, standarize=True, name=\"hepatitis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fa4271",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_messidor=np.array(df_messidor_features_cleaned.Class)\n",
    "X_messidor=np.array(df_messidor_features_cleaned.drop(\"Class\", axis=1))\n",
    "X_train_messidor_normalized, y_train_messidor_normalized, X_test_messidor_normalized, y_test_messidor_normalized = test_train_split(X_messidor, y_messidor, standarize=True, name=\"messidor\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446e1b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets see the best combination of functions and K-NN for hepatitis normalized: Random search\n",
    "range_of_neighbors=[k for k in range(1, 11)]\n",
    "validate_functions=[euclidean, cosine_similarity]\n",
    "best_variable_set_knn_hep=find_knn_best_params(range_of_neighbors, \n",
    "               X_train_hepatitis_normalized, y_train_hepatitis_normalized, \n",
    "               X_test_hepatitis_normalized, y_test_hepatitis_normalized, \n",
    "                                       validate_functions, weighted=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee98727",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"best combination for KNN on hepatitis data\")\n",
    "print(\"K: \", best_variable_set_knn_hep.k)\n",
    "if best_variable_set_knn_hep.func==\"dist\":\n",
    "    print(\"Cost function: Euclidian\")\n",
    "else:\n",
    "    print(\"Cost function: \", best_variable_set_knn_hep.func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56d55ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets do the same for messidar data\n",
    "range_of_neighbors=[k for k in range(1, 15)]\n",
    "validate_functions=[euclidean, cosine_similarity]\n",
    "best_variable_set_knn_mess=find_knn_best_params(range_of_neighbors, \n",
    "               X_train_messidor_normalized, y_train_messidor_normalized, \n",
    "               X_test_messidor_normalized, y_test_messidor_normalized, \n",
    "                                       validate_functions, weighted=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d7331a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"best combination for KNN on messidor data\")\n",
    "print(\"K: \", best_variable_set_knn_mess.k)\n",
    "if best_variable_set_knn_mess.func==\"dist\":\n",
    "    print(\"Cost function: Euclidian\")\n",
    "else:\n",
    "    print(\"Cost function: \", best_variable_set_knn_mess.func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb47535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now dt: for hepatitis normalized: search over k with random cost\n",
    "range_of_depths=[2,3,5]\n",
    "validate_min_leaf=[2,3,5]\n",
    "validate_functions=[cost_misclassification, cost_gini_index, cost_entropy]\n",
    "best_variable_set_dt_hep=find_optimal_set(range_of_depths, \n",
    "                X_train_hepatitis_normalized, X_test_hepatitis_normalized,\n",
    "                y_test_hepatitis_normalized, y_train_hepatitis_normalized, \n",
    "                 validate_functions, validate_min_leaf )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7912ce93",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"best combination for dt on hepatitis data\")\n",
    "print(\"max_depth: \", best_variable_set_dt_hep.max_depth)\n",
    "print(\"Cost function: \", best_variable_set_dt_hep.cost_fn)\n",
    "print(\"min_leaf_instances: \", best_variable_set_dt_hep.min_leaf_instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d13a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now dt: for mess normalized: search over max_depth with random cost and leaf\n",
    "range_of_depths=[2,3,5]\n",
    "validate_min_leaf=[2,3,5]\n",
    "validate_functions=[cost_misclassification, cost_gini_index, cost_entropy]\n",
    "best_variable_set_dt_mess=find_optimal_set(range_of_depths, \n",
    "                X_train_messidor_normalized, X_test_messidor_normalized,\n",
    "                y_test_messidor_normalized, y_train_messidor_normalized, \n",
    "                 validate_functions, validate_min_leaf )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9742ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"best combination for dt on messidor data\")\n",
    "print(\"max_depth: \", best_variable_set_dt_mess.max_depth)\n",
    "print(\"Cost function: \", best_variable_set_dt_mess.cost_fn)\n",
    "print(\"min_leaf_instances: \", best_variable_set_dt_mess.min_leaf_instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b41914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on the analysis above we will train four modelswith:\n",
    "# model1\n",
    "# k=3 for hep, function=eucldian (knn)\n",
    "# with [\"spiders\", \"sgot\", \"anorexia\", \"ascites\", \"antivirals\",] as features from hepatitis data\n",
    "# model 2\n",
    "# k=5 for messi, function=euclidian (knn)\n",
    "# with \"MA_detection_0.8\", \"MA_detection_0.7\",\"MA_detection_0.5\", \"MA_detection_0.6\", \"quality\", \"pre-screening\", \"diameter\"]\n",
    "# as features from messidor data\n",
    "# model 3\n",
    "# max_depth=2, min_leaf_instances=5, function=entropy, (dt)\n",
    "# with spiders, malaise, age, steroid, bilirubin, liverfirm as features from hepatitis data\n",
    "# model four\n",
    "# max_depth=2, min_leaf_instances=5, function=entropy,  (dt)\n",
    "# with [\"MA_detection_0.5\", \"MA_detection_0.9\",\"pre-screening\", \"MA_detection_1.0\"] as features from messidor data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83011147",
   "metadata": {},
   "outputs": [],
   "source": [
    "## model1\n",
    "y_hepatitis_1=np.array(df_hepatitis_cleaned.Class)\n",
    "X_hepatitis_1=np.array(df_hepatitis_cleaned[[\"spiders\", \"sgot\", \"anorexia\", \"ascites\", \"antivirals\"]])\n",
    "y_preds, yh=cross_validate(kNN(3, euclidean, function_type(euclidean), False), X_hepatitis_1, y_hepatitis_1, n_splits=5, standarize=True, columns_index=[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122a0f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix1=confusion_matrix(yh, y_preds, [1,2])\n",
    "print(\"Accuracy: \", evaluate_acc(yh, y_preds)*100, \"%\")\n",
    "print()\n",
    "print(\"Sensitivity: \", sensitivity(confusion_matrix1)*100, \"%\")\n",
    "print()\n",
    "print(\"precision: \", precision(confusion_matrix1)*100, \"%\")\n",
    "print()\n",
    "print(\"fpr: \", fpr(confusion_matrix1)*100, \"%\")\n",
    "print()\n",
    "print(\"specificity: \", specificity(confusion_matrix1)*100, \"%\")\n",
    "print()\n",
    "print(\"f1_score: \", f1_score(precision(confusion_matrix1),sensitivity(confusion_matrix1) )*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12bd596",
   "metadata": {},
   "outputs": [],
   "source": [
    "## model2\n",
    "y_messidor_2=np.array(df_messidor_features_cleaned.Class)\n",
    "X_messidor_2=np.array(df_messidor_features_cleaned[[\"MA_detection_0.8\", \"MA_detection_0.7\",\"MA_detection_0.5\", \"MA_detection_0.6\", \"quality\", \"pre-screening\", \"diameter\"]])\n",
    "y_preds, yh=cross_validate(kNN(5, euclidean, function_type(euclidean), False), X_messidor_2, y_messidor_2, n_splits=5, standarize=True, columns_index=[0,1,2,3,6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13eb6d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix2=confusion_matrix(yh, y_preds)\n",
    "print(\"Accuracy: \", evaluate_acc(yh, y_preds)*100, \"%\")\n",
    "print()\n",
    "print(\"Sensitivity: \", sensitivity(confusion_matrix2)*100, \"%\")\n",
    "print()\n",
    "print(\"precision: \", precision(confusion_matrix2)*100, \"%\")\n",
    "print()\n",
    "print(\"fpr: \", fpr(confusion_matrix2)*100, \"%\")\n",
    "print()\n",
    "print(\"specificity: \", specificity(confusion_matrix2)*100, \"%\")\n",
    "print()\n",
    "print(\"f1_score: \", f1_score(precision(confusion_matrix2),sensitivity(confusion_matrix2) )*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f69adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## model3\n",
    "y_hepatitis_3=np.array(df_hepatitis_cleaned.Class)\n",
    "X_hepatitis_3=np.array(df_hepatitis_cleaned[[\"bilirubin\", \"liver_firm\", \"age\", \"spiders\", \"malaise\", \"steroid\"]])\n",
    "y_preds, yh=cross_validate(DecisionTree(2, 2,cost_entropy,5), X_hepatitis_3, y_hepatitis_3, n_splits=5, standarize=True, columns_index=[0,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78a40d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix3=confusion_matrix(yh, y_preds, [1,2])\n",
    "print(\"Accuracy: \", evaluate_acc(yh, y_preds)*100, \"%\")\n",
    "print()\n",
    "print(\"Sensitivity: \", sensitivity(confusion_matrix3)*100, \"%\")\n",
    "print()\n",
    "print(\"precision: \", precision(confusion_matrix3)*100, \"%\")\n",
    "print()\n",
    "print(\"fpr: \", fpr(confusion_matrix3)*100, \"%\")\n",
    "print()\n",
    "print(\"specificity: \", specificity(confusion_matrix3)*100, \"%\")\n",
    "print()\n",
    "print(\"f1_score: \", f1_score(precision(confusion_matrix3),sensitivity(confusion_matrix3) )*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bd79e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## model 4\n",
    "y_messidor_4=np.array(df_messidor_features_cleaned.Class)\n",
    "X_messidor_4=np.array(df_messidor_features_cleaned[[\"MA_detection_0.5\", \"MA_detection_0.9\",\"pre-screening\", \"MA_detection_1.0\"]])\n",
    "y_preds, yh=cross_validate(DecisionTree(2, 2,cost_entropy,5), X_messidor_4, y_messidor_4, n_splits=5, standarize=True, columns_index= [0,1,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ec6e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix4=confusion_matrix(yh, y_preds)\n",
    "print(\"Accuracy: \", evaluate_acc(yh, y_preds)*100, \"%\")\n",
    "print()\n",
    "print(\"Sensitivity: \", sensitivity(confusion_matrix4)*100, \"%\")\n",
    "print()\n",
    "print(\"precision: \", precision(confusion_matrix4)*100, \"%\")\n",
    "print()\n",
    "print(\"fpr: \", fpr(confusion_matrix4)*100, \"%\")\n",
    "print()\n",
    "print(\"specificity: \", specificity(confusion_matrix4)*100, \"%\")\n",
    "print()\n",
    "print(\"f1_score: \", f1_score(precision(confusion_matrix4),sensitivity(confusion_matrix4) )*100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f602f87a",
   "metadata": {},
   "source": [
    "### Curve Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4d1ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_preds(y_probs, classes=[0,1]):\n",
    "    ypred = []\n",
    "    y_probs = list(y_probs)\n",
    "    for p in y_probs:\n",
    "        if p:\n",
    "            ypred.append(classes[1])\n",
    "        else:\n",
    "            ypred.append(classes[0])\n",
    "    return ypred\n",
    "\n",
    "def get_preds_thresholds(class_probs, classes=[0,1], thresholds=np.linspace(0, 1, 200)):\n",
    "    \"\"\"\n",
    "    Input \n",
    "        class_probs         Array of the probabilities of each point being in the given classes.\n",
    "                            Rows are number of data points, with 1 column per class, with values as the p(being in the class)\n",
    "        classes             Default [0,1], else whatever classes are in the data\n",
    "        thresholds          Default np.linspace(0, 1, 200), else whatever threshold range you want\n",
    "    Return\n",
    "        thresholded_preds   Class predictions based on threshold\n",
    "    \"\"\"\n",
    "#     thresholds[0]=-1\n",
    "    thresholded_preds=[]   \n",
    "    for thresh in thresholds:                   # go through each threshold\n",
    "        y_probs = class_probs[:,1] >= thresh    # get probs of true class\n",
    "        ypred = assign_preds(y_probs, classes)\n",
    "        thresholded_preds.append(ypred)\n",
    "    return thresholded_preds\n",
    "\n",
    "# roc \n",
    "def roc(thresholded_preds, ytrue, classes=[0,1]):\n",
    "    tprs, fprs= [], []\n",
    "    for ypred in thresholded_preds:\n",
    "        conf_mat=confusion_matrix(ytrue, np.array(ypred), classes)\n",
    "        tprs.append(sensitivity(conf_mat))\n",
    "        fprs.append(fpr(conf_mat))\n",
    "    return tprs, fprs\n",
    "\n",
    "# roc auc \n",
    "def roc_auc_score():\n",
    "    # didnt find a formula in the slides!\n",
    "    return    \n",
    "\n",
    "# Precision_recall_curve \n",
    "def Precision_recall_curve(thresholded_preds, ytrue, classes=[0,1]):\n",
    "    Prec, rec= [], []\n",
    "    for ypred in thresholded_preds:\n",
    "        conf_mat=confusion_matrix(ytrue, np.array(ypred), classes)\n",
    "        Prec.append(precision(conf_mat))\n",
    "        rec.append(sensitivity(conf_mat))\n",
    "    return Prec, rec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54529132",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true1, yh1=cross_validate(kNN(3, euclidean, function_type(euclidean), False), X_hepatitis_1, y_hepatitis_1, n_splits=5, standarize=True, columns_index=[1], prob=True)\n",
    "thresh1=get_preds_thresholds(yh1, [1,2])\n",
    "tprs1, fprs1=roc(thresh1, y_true1, [1,2])\n",
    "Prec1, rec1=Precision_recall_curve(thresh1, y_true1, [1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3212a9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true2, yh2=cross_validate(kNN(5, euclidean, function_type(euclidean), False), X_messidor_2, y_messidor_2, n_splits=5, standarize=True, columns_index=[0,1,2,3,6], prob=True)\n",
    "thresh2=get_preds_thresholds(yh2)\n",
    "tprs2, fprs2=roc(thresh2, y_true2)\n",
    "Prec2, rec2=Precision_recall_curve(thresh2, y_true2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3904b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true3, yh3=cross_validate(DecisionTree(2, 2,cost_entropy,5), X_hepatitis_3, y_hepatitis_3, n_splits=5, standarize=True, columns_index=[0,2], prob=True)\n",
    "thresh3=get_preds_thresholds(yh3, [1,2])\n",
    "tprs3, fprs3=roc(thresh3, y_true3, [1,2])\n",
    "Prec3, rec3=Precision_recall_curve(thresh3, y_true3,[1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92c11c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true4, yh4=cross_validate(DecisionTree(2, 2,cost_entropy,5), X_messidor_4, y_messidor_4, n_splits=5, standarize=True, columns_index= [0,1,3], prob=True)\n",
    "thresh4=get_preds_thresholds(yh4)\n",
    "tprs4, fprs4=roc(thresh4, y_true4)\n",
    "Prec4, rec4=Precision_recall_curve(thresh4, y_true4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959f1efb",
   "metadata": {},
   "source": [
    "### roc\n",
    "Only for messidor data because its balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337be8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "plt.plot(fprs2, tprs2, label='kNN: model2, messidor' )\n",
    "plt.plot(fprs4, tprs4, label='dt: model4, messidor' )\n",
    "\n",
    "plt.ylim(0.0,1.0)\n",
    "plt.xlim(0.0,1.0)\n",
    "\n",
    "ax.legend()\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29213569",
   "metadata": {},
   "source": [
    "### Prec-recall cruve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f49c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "plt.plot(rec4, Prec4, label='dt: model4, messidor' )\n",
    "\n",
    "plt.ylim(0.0,1.0)\n",
    "plt.xlim(0.0,1.0)\n",
    "\n",
    "ax.legend()\n",
    "plt.xlabel('Precision')\n",
    "plt.ylabel('Recall')\n",
    "plt.title('Precision-recall Curve')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f42b633",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b453540",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.16 64-bit ('3.8.16')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "c925766edaeb2e096b5ab9d3f363c193adbad705f69f6eaee42a6c9eedbbfe3b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
